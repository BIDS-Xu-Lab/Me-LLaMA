{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0390a82a-3fe3-484d-a4ba-d8777e74c047",
   "metadata": {},
   "source": [
    "# GENERAL DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3d0721-ec0e-4b75-9214-50173180bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lq62/.conda/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Huggingface token\n",
    "token = ''\n",
    "\n",
    "# Cache path\n",
    "local_cache_path = ''\n",
    "\n",
    "# Saved json file path\n",
    "saved_json_file = ''\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,token=token,)\n",
    "tokenizer.pad_token = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6a5467-02c2-41e9-a843-e9cc6fbb1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for togethercomputer/RedPajama-Data-1T contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    }
   ],
   "source": [
    "local_dataset = load_dataset('togethercomputer/RedPajama-Data-1T','arxiv',split = 'train',streaming=True,)\n",
    "shuffled_dataset = local_dataset.shuffle(seed = 42, buffer_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b784e6ef-dc75-4c98-9969-0a64526c6d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22060)\n",
      "tensor(54434)\n",
      "tensor(61043)\n",
      "tensor(109941)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "token = 0\n",
    "m = 0\n",
    "\n",
    "# token needed for general domain\n",
    "token_needed = 100000#14497666456\n",
    "\n",
    "with open(saved_json_file,'a') as fw:\n",
    "    for i in shuffled_dataset:\n",
    "\n",
    "        m+=1\n",
    "        if token <= token_needed: \n",
    "            text = i['text']\n",
    "            #token += i['token_count']\n",
    "            tokenized_batch = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "            token+=sum(sum(tokenized_batch['attention_mask'].T))\n",
    "            data = {'text': f'{text}'}\n",
    "            #print(token)\n",
    "            fw.write(json.dumps(data) + '\\n')\n",
    "\n",
    "            if m%1000000 == 0:\n",
    "                print(token/token_needed)\n",
    "\n",
    "        else:\n",
    "            print('done')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2dc883-12d3-43ed-b588-081316114c80",
   "metadata": {},
   "source": [
    "# BIO DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "328a2ce0-dee7-480d-9c5e-b695578031b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3748a7c2354539881b18a2c9950135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ca52c2c90f4fe3b85f0aadc1b64312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data1 = 'epfl-llm/guidelines'\n",
    "data2 = 'health360/Healix-Shot'\n",
    "\n",
    "local_dataset1 = load_dataset(data1,cache_dir = local_cache_path)['train']\n",
    "local_dataset2 = load_dataset(data2,cache_dir = local_cache_path)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d474e79c-7d62-453f-87ef-89a9c7339316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data1---epfl-llm/guidelines\n",
    "#data2---health360/Healix-Shot\n",
    "\n",
    "import json\n",
    "with open(saved_json_file,'a') as f:\n",
    "    for i in local_dataset1['train']:\n",
    "        \n",
    "        text = i['clean_text']  # for data1, use i['clean_text']; for data2, use i['text']\n",
    "        \n",
    "        data = {'text': f'{text}'}\n",
    "        f.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed0561-e69b-4dd0-aee2-c4169c7f9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text with jsonl format.\n",
    "# text3---the-pile-pubmed-central-refine-result.jsonl\n",
    "data3_path = ''\n",
    "\n",
    "import json\n",
    "with open(data3_path, 'r') as file, open(saved_json_file,'a') as fw:\n",
    "    k = 0\n",
    "    for line in file:\n",
    "        if k%50000 == 0:\n",
    "            print(1)\n",
    "        k+=1\n",
    "        data_line = json.loads(line)\n",
    "        \n",
    "        text = data_line['text']\n",
    "        data = {'text': f'{text}'}\n",
    "        fw.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93f82b-104e-4f6d-99f8-9b9137e9c5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22818d-d698-4ce0-a69c-37d5c501f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371f944-2f0e-42b2-9f1e-5fe2dfb24c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f79e1-1b25-4e04-bb53-abfa8e74861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e11ad3-804c-4d3d-8a64-a724680b5e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f84194-7075-462b-a840-68abdb02b046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
